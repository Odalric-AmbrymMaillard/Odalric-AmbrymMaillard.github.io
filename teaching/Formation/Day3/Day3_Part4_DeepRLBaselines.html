<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>INRIA Academy</title>
	<script src="mathjax-shortcuts.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>	
    <link rel="stylesheet" href="slides.css">
</head>
<body>
    <div class="slide-container">
<!-- Slide 1: Title Slide -->
<section class="slide intro">
    <h1>Deep Reinforcement Learning algorithms baselines</h1>
	
    <div>
		<p><strong>Odalric-Ambrym Maillard</strong></p>
		<p><strong>HORIBA</strong></p>
		<p><strong>July 09, 2025</strong></p>
	<img src="logos/Inria Academy.png" width="250px">
	</div>
</section>

<section class="slide intro">
    <h1>Roadmap</h1>	
	   <div>
	<ul>
	<li>Q-learning </li>
	<li>Deep Q-learning </li>
	<li>PPO</li>
	<li>SAC</li>
	</ul>
	</div>
</section>



<section class="slide intro">
    <h1>Q-learning</h1>
	<div>
	
<p class="highlight-box">
	Value function:  <br>
	\(V(s) = \max\limits_{a\in\cA}r(s,a) + \gamma \sum\limits_{s'}P(s'|s,a)V(s')\)<br>
	Quality function:<br>
\(	Q(s,a) = r(s,a) + \gamma \sum\limits_{s'}P(s'|s,a)\max\limits_{b\in\cA} Q(s',b)\)	
</p>
	</div>
	<div>
	
<p class="highlight-box">
	From trajectory \((s_t,a_t,r_t,s_{t+1},\dots)\), where  \(r_t\sim R(s_t,a_t), s_{t+1}\sim P(\cdot|s_t,a_t)\):<br>
	<l>Q-learning</l><br>
	
	\(Q_{t+1}(s_t,a_t) = (1-\alpha)Q_t(s_t,a_t) + \alpha \big(r_t + \gamma \max\limits_{b\in\cA} Q_t(s_{t+1},b)\big)\)
	</p>
<p>
Unbiased estimates?
</p>
	</div>
	<div>
	Unbiased estimates. <br>
	<l>Temporal difference</l>: \( \big[r_t + \gamma \max\limits_{b\in\cA} Q_t(s_{t+1},b)\big] - Q_t(s_t,a_t) \)
	</div>

</section> 
	
<section class="slide intro">
    <h1>Q-learning with function approximation</h1>
    <div>
	Deep Q Networks (<l>critic</l>)
		<p>
	We now parameterize the quality function \(\Q_\theta\).
	</p>
	</div>
	<div>
		Progressive update of the Quality function \(Q_\theta\) representation:
	<img src="fig3/DQN.png"  width="80%"><br>
	<p>
	Using a <l>Replay buffer</l> to reduce dependency between trajectory and Q estimates.
	</p>
	</div>

</section> 



<section class="slide technical">

    <h1>Proximal Policy Optimization principle</h1>    
	<div>
	PPO (<l>actor</l>)
	<p>
	We now parameterize the policy \(\pi_\theta\) instead of the Q function.
	</p>
	</div>
	
	<div>
	
	<p class="highlight-box">
	Progressive gradient step <br><br>
	
	\( \theta_{k+1}= \argmax\limits_{\theta} L_{\theta_k}(\theta)  - \lambda_k \Esp_{s\sim d[\pi_{\theta_k}]}[ \KL(\pi_{\theta_k}(s), \pi_{\theta}(s)) ] \)
	</p>
		</div>
	
	<div>
	<p>
	<ul>
	<li>	Base penalty L:
\(	L_\theta(\theta')= \Esp_{s\sim d[\pi_\theta],a\sim {\pi_\theta}}\bigg[  \frac{\pi_{\theta'}(s,a)}{{\pi_\theta}(s,a)}A^{\pi_\theta}(s,a)\bigg] \)</li>
<li>
Advantage function: \(A^\pi(s,a) = V^\pi(s)-Q^\pi(s,a)\)</li>
<li>
Asymptotic visitation frequency: \(d[\pi](s) = (1-\gamma)\lim\limits_{T\to\infty}  \sum\limits_{t=1}^T \gamma^{t-1}P_\pi^t(s|s_1)\)
</li>
</ul>
</p>
</div>
</section> 


<section class="slide intro">

    <h1>Proximal Policy Optimization Heuristics</h1>
	<div>
	<l>Be careful</l><br>
	In actual PPO, 
<ul>
<li>
1. \(A^\pi\) is <l>estimated</l> from trajectory of \(t\) observations: \(\hat A_t^\pi\)</li>
	
<li>
2. \(d[\pi]\) is estimated from <l>same</l> trajectory: \( \hat d_t[\pi]\)</li>

<li>
3.  They use <l>Incorrect</l> definition of \(d[\pi](s)\) wihout \(1-\gamma\) factor. </li>

<li>
4.  Regularisation \(\lambda_k\) is adapted at each step heuristically</li>

<li>
5. Replace loss \(L\) with  <l>clipping</l> function:<br>
\(L(\theta)=− \min(\tilde r(θ)A, \text{clip}(\tilde r(θ), 1−\epsilon, 1+\epsilon)A)\)
</li>
</ul>
</div>	
	
</section> 


<section class="slide intro">

    <h1>Soft Actor Critic</h1>
	<div>
	<l>Actor-Critic</l><br>
	Parameterize both Quality function \(Q_\theta\) and policy \(\pi_\omega\).
	</div>
	
	<div>
	High-level principle
	<p class="highlight-box">
	We change the objecitve to induce exploration, hence stability:<br>
	\( \pi^\star = \argmax\limits_\pi\, \sum\limits_{t} \Esp_{s_t,a_t}\bigg[{\bf r}(s_t,a_t)+ \alpha \mathcal{H}(\pi(\cdot|s_t))\bigg]\)
	<p>
	</div>
	<div>
	The <l>entropy regularisation</l> term \(\mathcal{H}\) favors stochastic policies (soft).
	</div>	
	<div>
	<p class="highlight-box">
	We can derive <l>modified Bellman</l> operator:<br><br>
	\(\mathcal{T}^\pi Q(s_t,a_t) = {\bf r}(s_t,a_t)+ \gamma\Esp_{s_{t+1}}[V(s_{t+1})]\) where<br>
	\(V(s_{t+1})= \Esp_{a_t\sim\pi}\bigg[ Q(s_t,a_t)-\alpha \log(\pi(a_t|s_t)\bigg] \)
	</p>
	</div>
	<div>
	Solution in \(\pi\) has explicit form, produces stochastic policies.
	</div>
	
</section> 



<section class="slide conclusion">
    <h2>MINI-QUIZZ</h2>
	
	
	<a href="https://app.wooclap.com/HORIBA3MINI/questionnaires/686d17f0907ee8632b260237">https://app.wooclap.com/HORIBA3MINI/questionnaires/686d17f0907ee8632b260237</a>
	
	<a href="Day3_Part1_PlanJourneeTeaser.html">Back</a>
</section>
    </div>
    
    <div class="controls">
        <button class="btn" id="prev">⬅ Previous</button>
        <button class="btn" id="next">Next ➡</button>
    </div>
    
    <script src="slides.js"></script>
</body>
</html>